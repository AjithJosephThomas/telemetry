input {
    pipeline {
        address => beats_pipeline
    }
}

filter {
    mutate {
        merge => { "@metadata" => "original_metadata" }
        remove_field => [ "original_metadata" ]
    }

    # !!! WARNING !!!
    # If you need to support multiple verions of beats agents, understand breaking schema changes for Beats agents between v6 and v7
    # https://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.0.html#id-1.8.8.19
    
    # This config copies condition fields we need for our output prefixes to metadata to accommodate v6/v7 breaking changes without having to duplicate outputs.

    # Event ID
    if [event_id] { #v6
        mutate {
            add_field => { "[@metadata][event_id]" => "%{[event_id]}" }
        }
    } else if [winlog][event_id] { #v7
        mutate {
            add_field => { "[@metadata][event_id]" => "%{[winlog][event_id]}" }
        }
    }
    mutate {
        convert => {
            "[@metadata][event_id]" => "integer"
        }
    }

    # Log name
    if [log_name] { #v6 Winlogbeat
        mutate {
            add_field => { "[@metadata][log_name]" => "%{[log_name]}" }
        }
    } else if [winlog][channel] { #v7 Winlogbeat
        mutate {
            add_field => { "[@metadata][log_name]" => "%{[winlog][channel]}" }
        }
    } else if [event][module] { #v7 Filebeat
        mutate {
            add_field => { "[@metadata][log_name]" => "%{[event][module]}" }
        }
    } else if [type] { #v7 Packetbeat and fail safe
        mutate {
            add_field => { "[@metadata][log_name]" => "%{[type]}" }
        }
    }


    # Replace all forward slashes with underscore to avoid problems with S3 partitioning.
    mutate {
        gsub => [
            "[@metadata][log_name]", "/", "_"
        ]
    }

    # Hostname
    if [beat][name] { #v6
        mutate {
            add_field => { "[@metadata][hostname]" => "%{[beat][name]}" }
        }
    } else if [agent][hostname] { #v7
        mutate {
            add_field => { "[@metadata][hostname]" => "%{[agent][hostname]}" }
        }
    } else { # Something went wrong to reach here
        mutate {
            add_field => { "[@metadata][hostname]" => "UNKNOWN" }
        }
    }



    ##############################################
    # SPLUNK FILTERING
    ##############################################
    # You can choose which events to duplicate off to Splunk by adding a "splunk" tag; these events are later cloned to a "splunk" type.

    # EXAMPLE Include ALL sysmon events:

    # if [@metadata][log_name] == "Microsoft-Windows-Sysmon_Operational" {
    #     mutate {
    #         add_tag => [ "splunk" ]
    #     }
    # }

    # EXAMPLE Include by Event ID:

    # if [@metadata][event_id] in [1102, 4103] {
    #     mutate {
    #         add_tag => [ "splunk" ]
    #     }
    # }

    # Misconfigurations and malfunctions can flood Splunk with events that risk breaching a license limit.
    # Tagging items with the "splunk_drop" tag will override the "splunk" tag and ensure that these events are not ingested.
    # ----
    # EXAMPLE Drop by username:

    # if [event_data][TargetUserName] == "DROP_EXAMPLE" {
    #     mutate {
    #         add_tag => [ "splunk_drop" ]
    #     }
    # }
    
    #---------------------------------------------  
    # /\ Add Splunk tag mutations above here. /\
    #---------------------------------------------  
    # You can choose to drop some unnecessary fields, to save on capacity and license limit in Splunk.
    # This pipline clones these events, so that it can send a different event to Splunk, without modifying the original that gets stored in S3.
    # This pipeline drop the original "plain text" log ([message]) and any logstash tags ([tags]) to save space.

    # if "splunk" in [tags] and "splunk_drop" not in [tags] {
    #     clone {
    #         clones => ["splunk"]
    #         remove_field => [ "message", "tags", "ecs", "agent" ]
    #     }
    # }

    ##############################################
}

output {
    if [type] != "splunk" {
        s3 {
            region => "${AWS_REGION}"
            bucket => "${BEATS_LOG_BUCKET}"
            size_file => "${S3_FILE_MAX_SIZE:2097152}"
            time_file => "${S3_FILE_MAX_TIME:2}"
            encoding => "gzip"
            codec => json_lines
            canned_acl => "bucket-owner-full-control"
            prefix => "beat=%{[@metadata][beat]}/version=%{[@metadata][version]}/name=%{[@metadata][hostname]}/log_name=%{[@metadata][log_name]}/year=%{+YYYY}/month=%{+MM}/day=%{+dd}/hour=%{+HH}"
            temporary_directory => "/tmp/logstash/s3/beats"
        }
    }

    # Choose which events to send to Splunk by checking for the "splunk" type, created by the clone filter.
    # This section forwards those events to a pre-configured Universal Forwarder.

    # if [type] == "splunk" {
    #     if [@metadata][log_name] == "Microsoft-Windows-Sysmon_Operational" {
    #         tcp {
    #             host => "${SPLUNK_FORWARDER}"
    #             port => "${SPLUNK_PORT_SYSMON_EVT}"
    #             codec => "json_lines"
    #         }
    #     }
    # }

    if [@metadata][DEBUG_OUTPUT] == "true" {
        stdout {
            codec => rubydebug {
                metadata => true
            }
        }
    }
}